#!/usr/bin/env python
# https://github.com/spro/char-rnn.pytorch

import torch
import torch.nn as nn
from torch.autograd import Variable
import argparse
import os
import numpy as np

import json

from tqdm import tqdm

from helpers import *
from .model import *
from .logistic_regression import *

import random_walks

def train_extractor(description, generator, sentences, output_location):
    """
    Train an extractor for the original states given a description file
    (as generated by `describe.py`), a WalkGraph `generator`, and the
    original dataset that the descirption file was generated on.

    Args:
        description (array of batches of hidden states): output of `describe.py`
        generator (WalkGraph): the walk graph to track hidden states on
        sentences (array of tuples of ints): the original dataset that the description is for

    Returns:

    """
    np.random.seed(128)

    def index_to_state_array(index):
        sentence = sentences[index]

        sentence = tuple(decoding_dict[c] for c in sentence)

        states = tuple(generator.reconstruct_hidden_states(sentence))

        return states

    def index_to_comparison_state_array(index):
        sentence = sentences[index]

        sentence = tuple(decoding_dict[c] for c in sentence)

        states = tuple(comparison.reconstruct_hidden_states(sentence))

        return states

    # TODO: create a random 0-drop-prob placebo generator
    # to make sure that we are better at predicting the real
    # one than the fake one.

    total_example_pool = []

    for batch in tqdm(description):
        indices, positions = batch

        labels = [
            index_to_state_array(index)
            for index in indices
        ]

        for j, position in enumerate(positions):
            cell, hidden = position

            sample = torch.cat([cell[0], cell[1],
                    hidden[0], hidden[1]], dim=1)
            # sample is now a batch_sizex400 tensor
            # we need to attach labels.

            label = torch.LongTensor(
                [labels[i][j] for i in range(sample.size(0))]
            )

            if sample.is_cuda:
                label = label.cuda()

            # One batch.
            total_example_pool.append((label, sample))

    extractor_model = LogisticRegression(400, generator.states)
    extractor_model = extractor_model.cuda()

    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(extractor_model.parameters(), lr=1e-5)

    MAX_EPOCHS = 50
    loss_history = []
    last_accuracy = 0

    best_loss = None

    for epoch in range(MAX_EPOCHS):
        for labels, sample in total_example_pool:
            optimizer.zero_grad()
            pred = extractor_model(sample)
            loss = criterion(pred, labels)
            loss.backward()
            optimizer.step()

        # Measure loss
        correct = 0
        total = 0
        total_loss = 0

        with torch.no_grad():
            for labels, sample in total_example_pool:
                pred = extractor_model(sample)

                total_loss += criterion(pred, labels)

                maxs, indices = torch.max(pred, dim=1)

                correct += (indices == labels).sum()

                total += indices.size(0)

        if best_loss is None or total_loss < best_loss:
            best_loss = total_loss
            print('Saving model as loss %f is better.' % total_loss)
            torch.save(
                    extractor_model,
                    output_location
                )

        last_accuracy = float(correct) / total

        print('EPOCH %d ACCURACY %d/%d (%f), loss %f' % (
            epoch, correct, total, float(correct) / total, total_loss / total))

        loss_history.append(total_loss / total)

        if len(loss_history) >= 5 and all(loss_history[-5] - x < 1e-6 for x in loss_history[-5:]):
            print('Breaking because not improving', loss_history[-5], loss_history[-5:])
            break


def run_extraction(description_file, generator_file, dataset_file):
    """
    Train an pair of extractors for a given description file;
    one that extracts the actual states, and one control one which extracts
    the states of a random graph.

    Writes the results to the same directory as the description file.

    Args:
        description_file (str): `.pt` location of the description file
        generator_file (str): `.json` location of the WalkGraph serialization
        dataset_file (str): `sentences` location of the sentences that the description file is for
    """
    description = torch.load(description_file)

    # Now description will be an array of tuples
    # (indices, record)
    # where indices is an array of indices into the dataset,
    # and record is an array of length seq_len of batched hidden states.

    # Open up the generator
    with open(generator_file) as f:
        generator = random_walks.load_from_serialized(json.load(f))

    # Now, we would like to read the real generator states
    # off the indices. Let's do that.

    file, file_len = read_file(dataset_file)

    sentences = file.split('\n')

    comparison = random_walks.create_random_walk_graph(
        generator.states,
        generator.alphabet_size,
        np.random,
        drop_prob = 0
    )

    model_location = os.path.split(description_file)[0]

    with open(os.path.join(model_location, 'comparison-graph.json'), 'w') as f:
        json.dump(comparison.serialize(), f)

    # Train
    train_extractor(description, generator, sentences,
        os.path.join(model_location, 'extractor-model-test.pt'))
    train_extractor(description, generator, sentences,
        os.path.join(model_location, 'extractor-comparison-test.pt'))
